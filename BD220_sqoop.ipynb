{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hh_demo = [age_desc: string, marital_status_code: string ... 6 more fields]\n",
       "product = [product_ID: int, manufacturer: int ... 5 more fields]\n",
       "transaction = [household_key: int, basket_id: bigint ... 10 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[household_key: int, basket_id: bigint ... 10 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/* SQOOP CODE TO GET THE TABLES\n",
    "\n",
    "\n",
    "sqoop import --driver com.microsoft.sqlserver.jdbc.SQLServerDriver --connect 'jdbc:sqlserver://bigdata220w18.database.windows.net:1433;database=week3' --username <user> --password \"<password>\" \\\n",
    " --query 'SELECT * FROM HH_DEMOGRAPHIC WHERE $CONDITIONS' \\\n",
    " --split-by household_key -m 1 --target-dir /user/elena/users/hh --fields-terminated-by '\\t' --lines-terminated-by '\\n' --as-parquetfile\n",
    " \n",
    "sqoop import --driver com.microsoft.sqlserver.jdbc.SQLServerDriver --connect 'jdbc:sqlserver://bigdata220w18.database.windows.net:1433;database=week3' --username <user> --password \"<password>\" \\\n",
    " --query 'SELECT * FROM transaction_data WHERE $CONDITIONS' \\\n",
    " --split-by basket_id -m 1 --target-dir /user/elena/users/trans --fields-terminated-by '\\t' --lines-terminated-by '\\n' --as-parquetfile\n",
    " \n",
    "sqoop import --driver com.microsoft.sqlserver.jdbc.SQLServerDriver --connect 'jdbc:sqlserver://bigdata220w18.database.windows.net:1433;database=week3' --username <user> --password \"<password>\" \\\n",
    " --query 'SELECT * FROM product WHERE $CONDITIONS' \\\n",
    " --split-by product_id -m 1 --target-dir /user/elena/users/product --fields-terminated-by '\\t' --lines-terminated-by '\\n' --as-parquetfile\n",
    " \n",
    " */\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 143:===================================================> (195 + 4) / 200]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hhDF = [age_desc: string, marital_status_code: string ... 6 more fields]\n",
       "prodDF = [product_ID: int, manufacturer: int ... 5 more fields]\n",
       "tranDF = [household_key: int, basket_id: bigint ... 10 more fields]\n",
       "rfm = [household_key: int, recency: int ... 2 more fields]\n",
       "tran_hh = [household_key: int, basket_id: bigint ... 11 more fields]\n",
       "rfm_hh = [household_key: int, recency: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[household_key: int, recency: int ... 2 more fields]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hhDF = spark.read.parquet(\"/user/elena/users/hh/*.parquet\").toDF\n",
    "val prodDF = spark.read.parquet(\"/user/elena/users/product/*.parquet\").toDF\n",
    "val tranDF = spark.read.parquet(\"/user/elena/users/trans/*.parquet\").toDF\n",
    "\n",
    "hhDF.createOrReplaceTempView(\"hhDF\")\n",
    "prodDF.createOrReplaceTempView(\"prodDF\")\n",
    "tranDF.createOrReplaceTempView(\"tranDF\")\n",
    "\n",
    "/* create an RFM table for all customers */\n",
    "val rfm = spark.sql(\"\"\"SELECT household_key, \n",
    "                              MAX(day) as recency, \n",
    "                              COUNT(DISTINCT basket_id) as frequency, \n",
    "                              SUM(sales_value) as monetary \n",
    "                       FROM tranDF \n",
    "                       GROUP BY household_key\"\"\")\n",
    "\n",
    "/* add homeownership info to transaction table */\n",
    "val tran_hh = spark.sql(\"\"\"SELECT a.*, b.homeowner_desc \n",
    "                          FROM tranDF a LEFT JOIN hhDF b \n",
    "                          ON a.household_key=b.household_key\"\"\")\n",
    "\n",
    "tran_hh.createOrReplaceTempView(\"tran_hh\")\n",
    "\n",
    "/* create an RFM table for homeowners */\n",
    "val rfm_hh = spark.sql(\"\"\"SELECT household_key, \n",
    "                              MAX(day) as recency, \n",
    "                              COUNT(DISTINCT basket_id) as frequency, \n",
    "                              SUM(sales_value) as monetary \n",
    "                       FROM tran_hh \n",
    "                       WHERE homeowner_desc = 'Homeowner'\n",
    "                       GROUP BY household_key\n",
    "                       \"\"\")\n",
    "\n",
    "rfm.write.csv(\"file:///data/rfm.csv\")\n",
    "rfm_hh.write.csv(\"file:///data/rfm_hh.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
